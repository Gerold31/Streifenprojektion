\documentclass[ngerman,a4paper,parskip=half]{scrartcl}

\usepackage[ngerman]{babel}
\usepackage[utf8]{inputenc}
\usepackage[onehalfspacing]{setspace}

\usepackage{helvet}
\usepackage[T1]{fontenc} % working hyphenation
\usepackage{lmodern} % better fonts with T1-fontenc
\usepackage{amsmath, amssymb, amstext}
\usepackage{subcaption}
\usepackage{float}
\usepackage[affil-it]{authblk}
\usepackage[round]{natbib}
\usepackage[nolist]{acronym}
%\usepackage{wrapfig}
\usepackage{fancyref}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{pgfplots}

\usepackage[hidelinks]{hyperref}
%\usepackage[left=3.2cm,right=3cm,top=1.7cm,bottom=3cm,includeheadfoot]{geometry}
\usepackage[left=3.2cm,right=3cm,top=2cm,bottom=2.5cm,includeheadfoot]{geometry}

\def \N{\mathbb{N}}
\def \Z{\mathbb{Z}}
\def \Q{\mathbb{Q}}
\def \R{\mathbb{R}}
\def \C{\mathbb{C}}
\def \fov{\mathrm{fov}}

\begin{acronym}[FOV]
	\acro{FOV}{Field of view}
\end{acronym}

\hypersetup{
	pdftitle    = {Streifenlichtprojektion und optische Analyse zur Oberflächeninspektion},
	pdfsubject  = {Streifenlichtprojektion},
	pdfauthor   = {Dennis~Wagner, Johannes~Spangenberg, Leroy~Kramer},
	pdfkeywords = {Streifenlichtprojektion, Humboldt Universität, Informatik, Semesterprojekt},
	%	pdfcreator  = {pdflatex},
	%	pdfproducer = {LaTeX with hyperref},
}

%Kopf- und Fußzeile
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}

%Kopfzeile links bzw. innen
\fancyhead[L]{\nouppercase{\leftmark}}
%Kopfzeile rechts bzw. außen
%\fancyhead[R]{\today}
%Linie oben
\renewcommand{\headrulewidth}{0.5pt}

%Fußzeile mittig
\fancyfoot[C]{\thepage}
%Linie unten
\renewcommand{\footrulewidth}{0.5pt}

\begin{document}

% ---------------------------------------------------------------------------- %

\input{Streifenprojektion_Title}
\tableofcontents
\newpage

% ---------------------------------------------------------------------------- %

\section{Einleitung}

In verschiedenen Fällen ist es hilfreich oder notwendig ein \emph{komplexes} dreidimensionales Objekt zu vermessen. Aus solchen Vermessungen resultierende Modelle können beispielsweise in der Unterhaltungsindustrie für die Film- und Spielproduktion verwendet werden. So lassen sich heutzutage dank solcher Verfahren vergleichsweise einfach Personen und Objekte in Spiele übernehmen. Außerdem ermöglichen Verfahren zur Vermessung von Geometrien automatisierte Qualitätskontrollen und neue Methoden zur automatisierten Fertigung oder Verarbeitung.

In diesem Projekt geht es um die \emph{Streifenlichtprojektion}. Dabei wird ein Streifen auf eine Oberfläche projiziert um aus einer Aufnahme der projizierten Linie die Form der Struktur zu rekonstruieren.

% ---------------------------------------------------------------------------- %

\section{Theoretische und technische Grundlagen}
\label{sec:basics}

Zusätzlich zur hier verwendeten Methode haben sich in den letzten Jahrzehnten viele verschiedene Techniken entwickelt, mit denen dreidimensionale Strukturen der realen Welt vermessen werden können. So existieren zusätzlich zur Streifenlichtprojektion Verfahren wie \emph{Stereo-Vision}, \emph{Structure from Motion}, \emph{Shape from Shading} und \emph{Time of Flight}. Die benötigten theoretischen Grundlagen überschneiden sich dabei bei einigen der Verfahren, es gibt aber auch einige Unterschiede. Im Folgenden wird auf wichtige Grundlagen eingegangen, die für das Projekt benötigt werden.

\subsection{Normalisierte Bildkoordinaten}
\label{sec:imagecoordinates}

Ein \emph{fotografisches Bild} kann als eine zweidimensionalen Matrix, die für jeden Pixel eine Farbe definiert, dargestellt werden. Dabei eignen sich die Indices des Bilder allerdings nur begrenzt zur Beschreibung von Positionen auf dem Bild. Existieren beispielsweise mehrere Inhaltsgleiche Bilder mit unterschiedlicher Abtastrate (Auflösung), so beschreiben die selben Indices auf jedem Bild eine inhaltlich andere Position. Aus diesem Grund werden in vielen Situationen \emph{normalisierte Bildkoordinaten} verwendet. Das Ziel der normalisierten Bildkoordinaten ist es, Positionen auf einem Bild unabhängig von der Auflösung ausdrücken zu können.

Als normalisierte Bildkoordinaten wird hier ein Tupel aus zwei reellen Zahlen $(u,v)$ verwendet. $v$ ist dabei aus dem Intervall $[-1,1]$. Der Wertebereich von $u$ ergibt sich entsprechend aus dem Seitenverhältnis $r$: $u \in [-r,r]$. Dabei sollte erwähnt werden, dass normalisierte Bildkoordinaten je nach Anwendungsgebiet auch anders definiert werden können. So kann es je nach Sachverhalt sinnvoll sein, dass für $u$ ebenfalls $u \in [-1,1]$ gilt oder, dass sich $u$ und $v$ im Wertebereich $[0,1]$ befinden.

Um aus den Indizes $(i,j)$ eines Pixels die normalisierten Bildkoordinaten $(u,v)$ zu berechnen, kann hier folgende Gleichung verwendet werden:
\[ \begin{pmatrix}
u \\ v
\end{pmatrix} = 2 \cdot \begin{pmatrix}
\frac{i r}{s_x - 1} \\
\frac{j}{s_y - 1}
\end{pmatrix} - \begin{pmatrix}
r \\ 1
\end{pmatrix} \]
Mit der Bildauflösung $(s_x, s_y)$ und dem Seitenverhältnis $r = s_x/s_y$.

\subsection{Perspektivische Projektion}
\label{sec:perspective}

Bei einer \emph{perspektivischen Projektion} werden dreidimensionale Punkte auf eine \emph{Bildebene} projiziert. Die Funktion entspricht dabei dem Modell der \emph{Lochkamera} und stellt eine Vereinfachung vieler realen Kameras da. Eine perspektivische Projektion wird durch den Augpunkt $O$ und die Bildebene definiert. Um einen Objektpunkt $X$ auf einen Punkt $X'$ in der Bildebene zu projizieren, wird der Schnittpunkt des \emph{Projektionsstrahls} durch $X$ und $O$ mit der Bildebene bestimmt. Der projizierte Punkt wird auch $X'$ \emph{Bildpunkt} genannt. Die Bildebene wird oft durch eine Blickrichtung der \emph{Kamera} und eine \emph{Brennweite} $f$ definiert. Die Blickrichtung der Kamera entspricht dabei der Normalen der Bildebenen und $f$ ist die Entfernung der Bildebene vom Augpunkt in Blickrichtung.

\begin{figure}
	\centering
	\begin{subfigure}{0.4\textwidth}
		\includegraphics[width=\textwidth]{includes/perspective}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.4\textwidth}
		\includegraphics[width=\textwidth]{includes/perspective2}
	\end{subfigure}
	\caption{Perspektivische Projektion}
	\label{fig:perspective}
\end{figure}

Oft wird die perspektivische Projektion in Verbindung mit Bildern, wie sie in \Fref{sec:imagecoordinates} beschrieben werden, verwendet, und nicht mit konkreten Bildpunkten. Dazu muss definiert werden, wo sich das Bild auf der Bildebene befindet. Eine offensichtliche Möglichkeit ist die Angabe der Höhe und Breite des Bildes auf der Bildebene, unter der Annahme, dass der am nächsten liegende Punkt der Bildebene zum Augpunkt in der Mitte des Bildes liegt. Wenn das Seitenverhältnis des Bildes beibehalten wird, reicht auch die Angabe der Höhe. Da es meistens nicht relevant ist, wo sich die Bildebene genau befindet, sofern jedem Punkt eine Projektionsgerade zugeordnet wird, kann die Brennweite nach belieben verändert werden, solange auch die Bildhöhe entsprechend angepasst wird. Aus diesem Grund reicht es oft, nur die Brennweite als Parameter für die Projektion zu nutzen, sofern die Bildhöhe zuvor fest definiert wurde.

Angenommen das Bild ist $2$ Einheiten hoch, der Augpunkt ist $O(0,0,0)$ und die Kamera schaut in Richtung der negativen $z$-Achse. Dann kann aus den normalisierten Bildkoordinaten $(u,v)$ der entsprechende Bildpunkt $X'$ ganz einfach über die folgende Gleichung bestimmt werden:
\[ \vec{x'} = \begin{pmatrix}
u \\ v \\ -f
\end{pmatrix} \]

Alternativ zur Brennweite und Bildhöhe wird auch oft das vertikale \ac{FOV} benutzt, um die Projektion zu definieren. Das vertikale \ac{FOV} ist der Winkel zwischen den obersten und untersten Projektionsgeraden des Bildes. Die Brennweite $f$, die Bildhöhe $g$ und das vertikalen \ac{FOV} $\fov$ stehen dabei in der folgende Beziehung:
\begin{align*}
	\fov = 2 \cdot \arctan \left( \frac{g}{2 f} \right)
	\Leftrightarrow f = \frac{g}{2 \tan\left(\frac{\fov}{2}\right)}
\end{align*}

\subsection{Triangulation}

Bei der Triangulation geht es darum, aus zwei Eckpunkten und den Innenwickeln eines Dreiecks den dritten Eckpunkt zu bestimmen. Hier wird der Objektpunkt auf der Laserlinie auf Basis eines Bildpunktes und der Laserausrichtung bestimmt.

\subsubsection{Modell und gegebene Werte}

\begin{figure}
	\centering
	\begin{subfigure}{0.55\textwidth}
		\includegraphics[width=\textwidth]{includes/triangulation3d}
		\caption{dreidimensionales Modell}
		\label{fig:triangulation3d}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.35\textwidth}
		\includegraphics[width=\textwidth]{includes/triangulation_skew_pitch}
		\caption{Ausrichtung der Laserebene L}
		\label{fig:triangulation_skew_pitch}
	\end{subfigure}

	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{includes/triangulation2d}
		\caption{vereinfachtes zweidimensionales Modell}
		\label{fig:triangulation2d}
	\end{subfigure}
	\caption{Modell des Setups}
\end{figure}

Als Modell für die Triangulation wird die Abstraktion, wie sie auf \Fref{fig:triangulation3d} zu sehen ist, verwendet. Es existiert eine perspektivischen Projektion eines Objektpunktes $X$ auf eine Bildebene $B$, wobei die Blickrichtung entlang der negativen $z$-Achse verläuft. Der Augpunkt $O$ liegt dabei im Koordinatenursprung. Der Bildpunkt wird mit $X'$ bezeichnet. Das projizierte Licht wird durch eine Ebene $L$ dargestellt, die die Position des Projektors, das heißt den Projektionspunkt $P$, und den Objektpunkt $X$ schneidet.

Die durch das Setup gegebenen Werte:

\begin{tabular}{lp{12cm}}
	$P$                &
		Position des Linienprojektors.\\[0.5em]
	$f$                &
		Brennweite
		(unter der Annahme, dass das Bild auf der Bildebene $2$ Einheiten hoch ist)\\[0.5em]
	$\theta$,$\delta$  &
		Winkel zur Ausrichtung der Ebene $L$. Die Ausgangslage der Ebene ist parallel zur $y$-$z$-Ebene. Die Ebene wird zunächst mit dem Winkel $\theta$ um den Vektor {\color{red} $(0,0,-1)^T$} gedreht. Danach mit dem Winkel $\delta$ um den Vektor {\color{red} $(-\sin(\theta), -\cos(\theta), 0)^T$}. Siehe auch \Fref{fig:triangulation_skew_pitch}.
\end{tabular}

Zudem ist der Bildpunkt durch normalisierte Bildkoordinaten $(u, v)$ gegeben. Da die Brennweite $f$ auf eine Bildhöhe von $2$ Einheiten angepasst ist, gilt wie in \Fref{sec:perspective} beschrieben $X'(u, v, -f)$.

\subsubsection{Bestimmung der Entfernung}

Als \emph{Entfernung des Objektpunktes} $h$ wird die negierte 3. Koordinate von $X$ bezeichnet.

Um diese Entfernung zu bestimmen, wird das geometrische Modell des Setups vereinfacht. Als erstes wird ein Basiswechsel auf die Orthogonalbasis
\[ \left\lbrace \begin{pmatrix}
\cos(\theta) \\ \sin(\theta) \\ 0
\end{pmatrix}, \begin{pmatrix}
0 \\ 0 \\ -1
\end{pmatrix}, \begin{pmatrix}
\sin(\theta) \\ \cos(\theta) \\ 0
\end{pmatrix} \right\rbrace \]
vorgenommen, um im folgenden nur die ersten beiden Dimensionen zu betrachten. Der konstruierte zweidimensionale Raum hat die besondere Eigenschaft, dass von der Ebene $L$ und der $x$-$y$-Ebene nur die Geraden $a$ und $c$ übrig bleiben. Eine weitere besondere Eigenschaft ist, dass der Abstand zwischen $X$ und der $x$-$y$-Ebene direkt von einen in den anderen Raum übernommen werden kann. So entspricht $h$ im vereinfachten Modell der 2. Koordinate von $X$. Ein beliebiger Vektor $(v_1, v_2, v_3)^T$ im dreidimensionalen Model entspricht im zweidimensionalen Modell dem Vektor $(\cos(\theta) \cdot v_1 - \sin(\theta) \cdot v_2, -v_3)^T$.

Die Geraden $a$ und $c$ bilden zusammen mit der Geraden $b$, welche durch die Punkte $O$, $X$ und $X'$ verläuft, ein Dreieck, dessen Eckpunkte $A$, $C = X$ und $B = O$ sind. Dies ist auch nochmal in \Fref{fig:triangulation2d} zu sehen. Angenommen die Koordinaten von $P$ und $X'$ in der zweidimensionalen Darstellung sind $(p_1,p_2)$ und $(x'_1, x'_2)$, so können die Winkel $\alpha$ und $\beta$ und die Länge der Strecke zwischen $A$ und $B$ wie folgt berechnet werden:
\begin{align*}
	\alpha &= \frac{\pi}{2} - \arctan\left(\frac{x'_1}{x'_2}\right)\\
	\beta &= \frac{\pi}{2} + \delta\\
	\overline{AB} &= p_1 + \tan(\delta) \cdot p_2
\end{align*}

Die Entfernung $h$ ergibt sich daraufhin aus der folgenden Gleichung:
\[ h = \frac{\overline{AB} \cdot \sin(\alpha) \cdot \sin(\beta)}{\sin(\pi - \beta - \alpha)} \]

\subsubsection{Bestimmung der Koordinaten}

Nachdem die Entfernung $h$ des Objektpunktes bekannt ist, kann der Objektpunkt $X$ aus dem Bildpunkt bestimmt werden:
\[ \vec{x} = \frac{h}{f} \cdot \vec{x'} = \frac{h}{f} \cdot \begin{pmatrix}
u \\ v \\ -f
\end{pmatrix} \]

% ---------------------------------------------------------------------------- %

\section{Aufbau und Algorithmen}

\subsection{Hardware}

Die Hardware besteht aus einer Webcam und einem Linienlaser, der auf einem Modellbauservo montiert ist. Laser und Servo werden von einem ''Spark Core'' Mikrocontroller gesteuert, welcher wiederum über USB mit Software kommuniziert.

\begin{figure}
	\centering
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{includes/hardware_schematic}
		\caption{schematische Darstellung}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{includes/hardware}
		\caption{reale Hardware}
	\end{subfigure}
	\caption{Aufbau der Hardware}
	\label{fig:hardware}
\end{figure}

\subsection{Softwarearchitektur}

Zur Implementierung der Software wurde die Programmiersprache \emph{C++} nach dem \emph{Standard von 2011} verwendet. Als Bibliotheken kamen \emph{OpenCV} und \emph{Qt} zum Einsatz.

\subsubsection{Grundlegende Datenstrukturen}

Zur Übertragung von Informationen zwischen den verschiedenen Komponenten werden die Datenstrukturen \texttt{Line}, \texttt{Reconstruction} und \texttt{DeviceConfiguration} verwendet. In \Fref{fig:classes_base} sind die entsprechenden Klassen zu sehen und in \Fref{tab:classes_base} werden ihre Funktionen beschrieben.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{includes/classdiagram_base.png}
	\caption{Klassendiagramm zu den grundlegenden Datenstrukturen}
	\label{fig:classes_base}
\end{figure}

\begin{table}
	\centering
	\begin{tabular}{lp{10cm}}
		\texttt{DeviceConfiguration} &
			Die Struktur \texttt{DeviceConfiguration} speichert dabei alle Werte zum Setup, die zur Rekonstruktion benötigt werden. Zusätzlich wird eine Transformationsmatrix mitgeführt, auf die später noch eingegangen wird.\\[1em]
		\texttt{Line}                &
			Die Datenstruktur \texttt{Line} entspricht einer Menge von Abtastungen der projizierten Linie. Sie Speichert dabei zusätzlich zu einer Liste von Abtastungen (Instanzen von \texttt{Sample}) die Auflösung des zugrunde liegenden Bildes.\\[1em]
		\texttt{Reconstruction}      &
			Die Klasse \texttt{Reconstruction} dient zum Speichern des endgültigen Ergebnises der Rekonstruktion. Sie speichert somit eine Liste von rekonstruierten Punkten (Instanzen von \texttt{Point}). Zusätzlich hat die Klasse die Aufgabe, hinzugefügte rekonstruierte Punkte zur Ausgabe des Programms hinzuzufügen.
	\end{tabular}
	\caption{Funktion der grundlegenden Datenstrukturen}
	\label{tab:classes_base}
\end{table}

\subsubsection{Programmablauf}

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{includes/classdiagram_control.png}
	\caption{Klassendiagramm zu den Steuereinheiten}
	\label{fig:classes_control}
\end{figure}

Nach Programmstart wird als erstes die Funktion \texttt{Configuration::init(int,char**)} aufgerufen. Diese analysiert die Argumente, die dem Programm übergeben wurden, und speichert das Ergebnis in den statischen Feldern der Klasse. Unter anderem wird dabei jeweils eine Instanz einer Spezialisierung von \texttt{Controller}, \texttt{LightBarDetector} und \texttt{Reconstructor} in den Feldern von \texttt{Configuration} abgelegt. Als nächstes wird die Methode \texttt{main()} der \emph{Steuerungsklasse}, also der Spezialisierung von \texttt{Controller}, aufgerufen, welche den Rest des Programmablaufes steuert.

\subsection{Steuerungsklassen}

\begin{figure}
	\centering
	\includegraphics[width=0.4\linewidth]{includes/software_controller}
	\caption{Aktivitätsdiagramm zum Grundlegenden Programmablauf in den Steuerklassen.}
	\label{fig:software_controller}
\end{figure}

Der Ablauf, der von den verschiedenen Spezialisierungen von \texttt{Controller} vorgegeben wird, verfolgt dabei immer einen ähnlichen Ablauf, wie er auch in \Fref{fig:software_controller} beschrieben wird. Der Controller durchläuft eine Schleife und beschafft sich in jedem Schleifendurchlauf die aktuelle Gerätekonfiguration (\texttt{DeviceConfiguration}), eine Aufnahme durch das entsprechende Setup und ggf. ein aktuelles Referenzbild. Danach übergibt es die Steuerung an die Linienerkennung (\texttt{LightBarDetextor}) und dessen Ergebnis wird an den Rekonstruktionsalgorithmus (\texttt{Reconstructor}) übergeben.

Es existieren drei Spezialisierungen von \texttt{Controller}. Die einfachste Implementierung ist der \texttt{CommandController}, welcher Standardmäßig verwendet wird. Die Klasse liest die Standardeingabe oder die in den Argumenten definierte Datei Zeilenweise aus. Eine Zeile entspricht dabei einem Befehl, welcher ausgewertet und ausgeführt wird, bevor sich das Programm der nächsten Zeile zuwendet. In der \Fref{tab:commands} können alle Befehle nachgelesen werden.

\begin{table}[p]
	\centering
	\begin{tabular}{lp{10cm}}
		\bfseries Befehl             & \bfseries Beschreibung\\
		\hline\hline
		\texttt{b <Datei>}           &
			Diese Zeile definiert die angegebene \emph{Datei} als neues Referenzbild.\\
		\hline
		\texttt{fov <Winkel>}        &
			Hier wird das \ac{FOV} auf den angegebenen \emph{Winkel} gesetzt. Der Winkel muss dabei in Radiant angegeben werden.\\
		\hline
		\texttt{pp <x> <y> <z>}      &
			Der Befehl setzt die Projektorposition $P$ auf den Punkt $(x,y,z)$.\\
		\hline
		\texttt{po <skew> <pitch>}   &
			Setzt die Werte $\theta$ und $\delta$ aus dem Modell. Die Winkel müssen dabei in Radiant angegeben werden.\\
		\hline
		\texttt{t <$m_1$> \dots <$m_{16}$>} &
			Setzt die Transformationsmatrix. Dabei wird die Matrix Zeilenweise aufgefüllt:
			\[ \begin{pmatrix}
				m_{1}  & m_{2}  & m_{3}  & m_{4}\\
				m_{5}  & m_{6}  & m_{7}  & m_{8}\\
				m_{9}  & m_{10} & m_{11} & m_{12}\\
				m_{13} & m_{14} & m_{15} & m_{16}
			\end{pmatrix} \]\\
		\hline
		\texttt{c <x> <fov> <pitch>} &
			Setzt die Projektorposition $P$ auf $(x,0,0)$, das \ac{FOV} auf den angegeben Winkel $\fov$, $\delta$ auf den angegeben Wert von \emph{pitch} und $\theta = 0$.\\
		\hline
		\texttt{<Datei>}             &
			Lässt die Rekonstruktion auf Basis der angegebenen Aufnahme laufen.\\
		\hline\hline
	\end{tabular}
	\caption{Unterstützte Befehle der Steuerungsklasse \texttt{CommandController}.}
	\label{tab:commands}
\end{table}

Die Klassen \texttt{RotateScanController} und \texttt{TranslateScanController} beschaffen die Aufnahmen hingehen direkt von der Hardware. Der \texttt{RotateScanController} rotiert dabei den Streifenprojektor um passende Aufnahmen zu erhalten. Der \texttt{TranslateScanController} macht eine Aufnahme und wartet anschließend auf eine Bestätigung des Benutzers, dass sich das zu scannende Objekt um eine vorher definierte Strecke bewegt hat, bevor er das nächste Bild aufnimmt.

\subsection{Ansteuerung der Hardware}

\subsubsection{Ansteuerung des Mikrocontrollers}

Die Software steuert den Mikrocontroller über USB, wobei sich dieser als virtuellen COM-Port ausgibt. Zur Kommunikation wird ein einfaches Protokoll verwendet, das 2 Byte lange Nachrichten an den Mikrocontroller sendet. Das erste Byte gibt den Befehl an, das zweite den Parameter.\\

\begin{tabular}{|c|c|c|}
\hline
1. Byte & 2. Byte & Beschreibung \\
\hline
'm'\footnotemark & $\alpha \in [0,180]$ & Setzt die Servoposition auf $\alpha ^\circ$.\\
\hline
'l' & '0' oder '1' & Schaltet den Laser an ('1') bzw. aus ('0')\\
\hline
\end{tabular}\\

\footnotetext{Zeichen in Anführungszeichen stehen für den ASCII-Wert des Zeichens}

Die Klasse \texttt{Serial} ermöglicht es Nachrichten an den Mikrocontroller zu senden.

Die Klassen \texttt{Servo} und \texttt{Laser} der Steuerung von dem Servo bzw. Laser. Dabei werden Nachrichten in o.g. Form an den Mikrocontroller gesendet.

\subsubsection{Ansteuerung der Kamera}

Für das Auslesen von Bildern der Kamera ist die Klasse \texttt{Camera} zuständig, dabei liest ein extra Thread permanent Bilder von der Kamera.

Wenn ein anderer Thread die \texttt{capture}-Methode von \texttt{Camera} aufruft, blockiert er solange, bis der extra Thread das nächste Bild gelesen hat, und gibt dieses dann zurück. Das wird gemacht um sicherzustellen, dass immer das aktuellste Bild ausgelesen wird und nicht ein älteres Bild, das in einem Buffer der Kamera gespeichert wurde.


\subsection{Linienerkennung}

Zuerst wird ein Binärbild erzeugt, wobei der Algorithmus entscheidet, welche Pixel zur Linie gehören und welche nicht. Im zweiten Schritt wird jeder Pixel des Bild durchlaufen, wobei in die Datenstruktur \texttt{Line} die vermeintliche Position der Linie und deren Breite geschrieben wird. Die hier implementierten Algorithmen unterscheiden sich lediglich in der Erzeugung des Binärbildes.

\subsubsection{Differenzbildung (Diff)}

Bei diesem Verfahren muss für jedes zu analysierende Bild ein Referenzbild ohne die projizierte Linie gegeben sein. Dabei wird die Differenz für jedes Bild mit dem Referenzbild berechnet und mit Hilfe eines einfachen Filters in ein Binärbild umgewandelt.

\subsubsection{Farbfilter (Free)}

Für jeden Farbkanal wird einzeln der Mittelwert über das gesamte Bild berechnet, aus dem ein Schwellenwert für jeden Kanal einzeln bestimmt wird. Für jeden Pixel wird der Wert aller Farbkanäle mit dem Schwellenwert verglichen. Ist ein Wert kleiner als der entsprechende Schwellenwert, gehört der Pixel nicht zur Linie.

\subsection{Rekonstruktion}

Nachdem die Laserlinie erkannt wurde, werden die vermeintlichen Pixel der erkannten Linie in Objektpunkte überführt. Dies geschieht durch die Klasse \texttt{DefaultReconstructor}, welche derzeit die einzige Spezialisierung von \texttt{Reconstructor} darstellt.

Dazu werden die Formeln aus \Fref{sec:basics} angewendet, um für jeden Pixel zunächst die normalisierten Bildkoordinaten, die Entfernung $h$ und danach den Objektpunkt $X$ zu bestimmen. Daraufhin werden die erhalten Punkte mithilfe der Transformationsmatrix transformiert und an die Klasse \texttt{Reconstruction} weiter gegeben, welche sie letztendlich ausgibt.

%\subsection{Übersicht über alle Klassen}

\begin{figure}[p]
	\centering
	\includegraphics[width=\linewidth]{includes/classdiagram}
	\caption{Umfassendes Klassendiagramm}
	\label{fig:classes_all}
\end{figure}
	
%\begin{table}[p]
%	\centering
%	\begin{tabular}{lp{10cm}}
%		\bfseries Klasse                & \bfseries Funktion\\
%		\hline\hline
%		\texttt{CommandController}      &
%			Diese Klasse stellt als Spezialisierung von \texttt{Controller} eine Steuerungsklasse da, welche eingehende Befehle verarbeitet. Eine Liste der Befehl ist in \Fref{tab:commands} zu finden.\\
%		\hline
%		\texttt{Configuration}          &
%			Diese Klasse ist eine rein statische Klasse. Sie analysiert nach Programmstart die Argumente, die an das Programm übergeben wurden, und speichert die Einstellungen für die weitere Programmausführung.\\
%		\hline
%		\texttt{Controller}             &
%			Diese abstrakte Klasse bildet die Basis für alle Steuerungsklassen und definiert Funktionen mit denen die Steuerungsklasse ihre Arbeit verrichten kann. Eine Steuerungsklasse bestimmt den Programmablauf und ist dabei für die Beschaffung der Setup-Informationen und Bilder verantwortlich.\\
%		\hline
%		\texttt{DefaultReconstructor}   &
%			Als Spezialisierung von \texttt{Reconstructor} stellt diese Klasse den einzigen Rekonstruktionsalgorithmus da.\\
%		\hline
%		\texttt{DeviceConfiguration}    &
%			Diese Struktur speichert alle Werte zum Setup, die zur Rekonstruktion benötigt werden. Dazu gehört beispielsweise die Position und Ausrichtung des Projektors. Zusätzlich wird eine Transformationsmatrix mitgeführt, auf dessen Basis Punkte nach der Triangulation transformiert werden.\\
%		\hline
%		\texttt{DiffLightBarDetector}   &
%			Als Spezialisierung von \texttt{LightBarDetector} stellt diese Klasse eine Linienerkennung da.  Nähere Informationen in \Fref{sec:line_diff}.\\
%		\hline
%		\texttt{FreeLightBarDetector}   &
%			Als Spezialisierung von \texttt{LightBarDetector} stellt diese Klasse eine Linienerkennung da. Nähere Informationen in \Fref{sec:line_free}.\\
%		\hline
%		\texttt{LightBarDetector}       &
%			Diese abstrakte Klasse dient als Basis für alle Implementierungen zur Detektion der Laserlinie.\\
%		\hline
%		\texttt{Line}                   &
%			Diese Datenstruktur entspricht einer Menge von Abtastungen der projizierten Linie. Sie Speichert dabei eine Liste von Indices, dessen Pixel zur Laserlinie gehören, und die Auflösung des zugrunde liegenden Bildes. Die Indices werden dabei in Instanzen von \texttt{Sample} abgelegt.\\
%		\hline
%		\texttt{Reconstruction}         &
%			Diese Klasse speichert das endgültige Ergebnis der Rekonstruktion. Außerdem kümmert sie sich um die Ausgabe der rekonstruierten Punkte.\\
%		\hline
%		\texttt{Reconstructor}          &
%			Diese abstrakte Klasse dient als Basis für alle Implementierungen zur Rekonstruktion der Objektpunkte auf Basis der bereits erkannten Laserlinie.\\
%		\hline
%		\texttt{RotateScanController}   &
%			Diese Klasse stellt als Spezialisierung von \texttt{Controller} eine Steuerungsklasse da. Sie kann verwendet werden, um Objekte mit Hilfe einer Drehung des Projektors zu scannen.\\
%		\hline
%		\texttt{TranslateScanController} &
%			Diese Klasse stellt als Spezialisierung von \texttt{Controller} eine Steuerungsklasse da. Sie kann verwendet werden, um Objekte mit Hilfe einer schrittweisen Bewegung an der Kamera vorbei zu scannen.\\
%		\hline\hline
%	\end{tabular}
%	\caption{Umfassende Tabelle der Klassen}
%	\label{tab:classes_all}
%\end{table}

% ---------------------------------------------------------------------------- %

\section{Auswertung}

Im Folgenden werden verschiedene Seiten eines ''Rubik's Cube'' auf verschiedene Entfernungen gescannt. Es wurden die rote, grüne, blaue, gelbe und orange Seite in 300mm Entfernung, sowie die blaue Seite in 200mm, 400mm und 500mm Entfernung vermessen. Die Zugehörigkeit eines Messpunktes wird anhand seiner Farbe, die aus dem Referenzbild entnommen wurde, bestimmt. Da die Scans vor weißem Hintergrund durchgeführt wurden, wurde die Weiße Seite des ''Rubik's Cube'' nicht gescannt.

\subsection{''diff'' - Linienerkennung}
\label{subsec:auswertung.diff}


\begin{figure}
	\centering
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{includes/diff_orange_300_cam.png}
		\caption{komplette Punktwolke aus Sicht der Kamera}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{includes/diff_only_orange_300_cam.png}
		\caption{Punktwolke der Punkte, die zum Würfel gehören, aus Sicht der Kamera}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{includes/diff_orange_300_pos1.png}
		\caption{komplette Punktwolke von der Seite}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{includes/diff_only_orange_300_pos1.png}
		\caption{Punktwolke der Punkte, die zum Würfel gehören, von der Seite}
	\end{subfigure}
	\caption{Punktwolken eines Scans mit diff-Linienerkennung der Orangen Seite des ''Rubic's Cube'' in 300mm Entfernung}
\end{figure}

\begin{table}
	\begin{tabular}{|c|c|c|c|}
		\hline
		Wiederholung & Anzahl & Avg & Stdabw \\
		\hline
		1 & 1274 & 335.522 & 104.71 \\
		\hline
		2 & 1227 & 335.939 & 100.287 \\
		\hline
		3 & 1320 & 335.866 & 113.613 \\
		\hline
		4 & 1314 & 335.699 & 93.5961 \\
		\hline
		5 & 1275 & 336.151 & 96.484 \\
		\hline
		6 & 1332 & 336.996 & 82.5333 \\
		\hline
		7 & 1308 & 336.126 & 95.4918 \\
		\hline
		8 & 1300 & 335.716 & 96.6604 \\
		\hline
		9 & 1262 & 334.981 & 98.4233 \\
		\hline
		10 & 1299 & 334.636 & 90.3444 \\
		\hline
	\end{tabular}
	\caption{Scans der blauen Seite in 300mm Entfernung mit diff-Linienerkennung}
	\label{tab:diff_reps}
\end{table}

\begin{table}
	\begin{tabular}{|c|c|c|c|}
		\hline
		Farbe & Anzahl & Avg & Stdabw \\
		\hline
		Rot & 11276 & 339.363 & 82.0992 \\
		\hline
		Gruen & 14696 & 336.565 & 161.644 \\
		\hline
		Blau & 12911 & 335.714 & 97.4973 \\
		\hline
		Gelb & 14013 & 336.297 & 105.483 \\
		\hline
		Orange & 16358 & 339.817 & 44.8436 \\
		\hline
	\end{tabular}
	\caption{Scans verschiedener Seiten in 300mm Entfernung mit diff-Linienerkennung}
	\label{tab:diff_colors}
\end{table}

\begin{table}
	\begin{tabular}{|c|c|c|c|}
		\hline
		Entfernung & Anzahl & Avg & Stdabw \\
		\hline
		200 & 26351 & 226.706 & 50.0151 \\
		\hline
		300 & 12911 & 335.714 & 97.4973 \\
		\hline
		400 & 8353 & 455.418 & 158.489 \\
		\hline
		500 & 4810 & 566.892 & 146.983 \\
		\hline
	\end{tabular}
	\caption{Scans der blauen Seite in verschiedenen Entfernung mit diff-Linienerkennung}
	\label{tab:diff_dists}
\end{table}

In Tabelle \ref{tab:diff_reps} werden die zehn Scans der blauen Seite in 300mm Entfernung einzeln betrachtet, in Tabelle \ref{tab:diff_colors} werden die Scans der verschiedenen Seiten in 300mm aufgeführt, dabei werden jeweils alle Punkte aus allen zehn Scans zusammen betrachtet und in Tabelle \ref{tab:diff_dists} sind die Ergebnisse der Scans der blauen Seite in verschiedenen Entfernungen.

In den Tabellen stehen die Anzahl der Punkte, die zum Würfel gehören (''Anzahl''), das $20 \%$ gestutzte Mittel (Avg) und die Standardabweichung der gemessenen Entfernungen (Stdabw).


\begin{tikzpicture}
	\begin{axis}[
    	width=\linewidth,
    	line width=1pt,
    	xlabel={Entfernung},
    	ylabel={Häufigkeit},
    ]
		\addplot+[blue] table [x=dist, y=count] 
			{includes/histogram_diff_rotate_blue_normal_300.csv};
	\end{axis}
\end{tikzpicture}
	
\subsection{''free'' - Linienerkennung}


\begin{figure}
	\centering
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{includes/free_orange_300_cam.png}
		\caption{komplette Punktwolke aus Sicht der Kamera}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{includes/free_only_orange_300_cam.png}
		\caption{Punktwolke der Punkte, die zum Würfel gehören, aus Sicht der Kamera}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{includes/free_orange_300_pos1.png}
		\caption{komplette Punktwolke von der Seite}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{includes/free_only_orange_300_pos1.png}
		\caption{Punktwolke der Punkte, die zum Würfel gehören, von der Seite}
	\end{subfigure}
	\caption{Punktwolken eines Scans mit free-Linienerkennung der Orangen Seite des ''Rubic's Cube'' in 300mm Entfernung}
\end{figure}

\begin{table}
	\begin{tabular}{|c|c|c|c|}
		\hline
		Wiederholung & Anzahl & Avg & Stdabw \\
		\hline
		1 & 617 & 338.901 & 3.51507 \\
		\hline
		2 & 604 & 339.129 & 3.03182 \\
		\hline
		3 & 592 & 338.799 & 2.58397 \\
		\hline
		4 & 629 & 338.871 & 3.02885 \\
		\hline
		5 & 618 & 338.894 & 3.13461 \\
		\hline
		6 & 621 & 338.641 & 2.82711 \\
		\hline
		7 & 618 & 338.782 & 2.50554 \\
		\hline
		8 & 603 & 338.857 & 2.99073 \\
		\hline
		9 & 588 & 339.026 & 3.15995 \\
		\hline
		10 & 586 & 339.025 & 2.88505 \\
		\hline
	\end{tabular}
	\caption{Scans der blauen Seite in 300mm Entfernung mit free-Linienerkennung}
	\label{tab:free_reps}
\end{table}

\begin{table}
	\begin{tabular}{|c|c|c|c|}
		\hline
		Farbe & Anzahl & Avg & Stdabw \\
		\hline
		Rot & 6314 & 337.704 & 42.0001 \\
		\hline
		Gruen & 6957 & 338.864 & 25.3341 \\
		\hline
		Blau & 6076 & 338.888 & 2.98687 \\
		\hline
		Gelb & 13202 & 338.809 & 81.868 \\
		\hline
		Orange & 7649 & 337.439 & 30.9114 \\
		\hline
	\end{tabular}
	\caption{Scans verschiedener Seiten in 300mm Entfernung mit free-Linienerkennung}
	\label{tab:free_colors}
\end{table}

\begin{table}
	\begin{tabular}{|c|c|c|c|}
		\hline
		Entfernung & Anzahl & Avg & Stdabw \\
		\hline
		200 & 10738 & 224.81 & 2.87541 \\
		\hline
		300 & 6076 & 338.888 & 2.98687 \\
		\hline
		400 & 3259 & 458.838 & 5.33122 \\
		\hline
		500 & 2061 & 581.937 & 9.37247 \\
		\hline
	\end{tabular}
	\caption{Scans der blauen Seite in verschiedenen Entfernung mit free-Linienerkennung}
	\label{tab:free_dists}
\end{table}

Die Tabellen \ref{tab:free_reps}, \ref{tab:free_colors} und \ref{tab:free_dists} sind analog zu den Tabellen in \ref{subsec:auswertung.diff} für die ''free''-Linienerkennung aufgebaut.

\begin{tikzpicture}
	\begin{axis}[
    	width=\linewidth,
    	line width=1pt,
    	xlabel={Entfernung},
    	ylabel={Häufigkeit},
    ]
		\addplot+[blue] table [x=dist, y=count] 
			{includes/histogram_free_rotate_blue_normal_300.csv};
	\end{axis}
\end{tikzpicture}

% ---------------------------------------------------------------------------- %

\section{Zusammenfassung}

% ---------------------------------------------------------------------------- %

\section{Quellenverzeichnis}

% ---------------------------------------------------------------------------- %

\end{document}
